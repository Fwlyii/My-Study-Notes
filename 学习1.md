# 实验室第一次任务
## 前置配环境等
- SSH 就是让你在宿舍“远程遥控”机房那台电脑
- vim： 相当于 Windows 里的**记事本**。你要写代码或者改配置，没法右键“用记事本打开”，只能输入 vim 文件名。坑点提醒： 这个记事本进去后不能直接打字，要按一下 i 键才能开始写（Insert模式）。写完想走，要按 Esc 键，然后输入 :wq（Write & Quit，保存并退出）。
- 弄懂“环境管理”（你的专属工具箱)：Conda 的作用： 它帮你建很多个相互隔离的“小房间”（虚拟环境）。在这个房间里，你爱装什么版本的 Python 都可以，绝对不会影响隔壁房间。
## python基础语法
1. 列表 (List) []： 就像一个排好队的队伍。
2. 字典 (Dictionary) {}： 就真的是一本字典。比如文档里的：hparams = { "n_vocab": 50257, "n_embd": 768 }。这就是告诉电脑：查 n_vocab 这个词，对应的值是 50257。
## numpy基础语法
1. 核心概念：Shape（形状/维度）
文档里反复出现像 [1024, 768] 这样的数字 。通俗理解： 想象一个 Excel 表格。1024 是行数（Rows）：代表你一次输入了多少个字（Token）。768 是列数（Columns）：代表每个字有多少个“特征数值”来描述它。为什么重要？ 就像搭积木，如果一块积木的“形状”是圆的，另一块是方的，它俩就拼不到一起。在运行代码报错时，90% 的情况都是“形状对不上”（比如 Shape mismatch）。
2. 核心概念：矩阵乘法（符号 @）
你在代码里会看到 x @ w + b 这样的公式 。通俗理解： 这是 AI 思考的本质。x 是你的输入（比如“今天天气”）。w (Weight) 是模型的权重（它脑子里的知识）。@ 就是把输入和知识进行“搅拌”运算，最后得出一个结果。不用深究数学： 你只需要知道，@ 是 NumPy 特有的符号，专门用来做这种复杂的“搅拌”运算的。
## self-attention李宏毅
-  **输入**：之前我们接触到的都是输入一个向量，输出一个向量等。但是，一个句子可以拆解为多个向量。word embedding 就是把一个句子拆解为多个向量，一段声音信号也可以拆解为多个向量，一个人际网、一个分子也可以拆成向量。
-  **输出**：1.输入和输出长度相等，输出的label和输入的数量相等。这里以这个为例子，有同一个句子里面有I saw a saw，前后两个词词性不一样，大模型没有理由给两个输出的后文不一样。！！！！因此引入后面的self-attention 2.只输出一个label，如this is good变成一个label：good 3.输入和输出长度不相等，且输出的长度是由机器自己决定的 
-  **self-attention**：来考虑一整个句子，可以有多层。是transformer一个很重要的部分。输入几个向量，输出几个向量。怎么产生生成的向量：（记输入的向量分别为b1b2b3b4,输出的相应为a1a2a3a4）,现在以b1到a1为例，计算a1的时候，需要计算b1和其余b的相关性，计算相关性有多种方法。