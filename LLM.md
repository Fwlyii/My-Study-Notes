# 模型定义
- LLM（大语言模型）是通过训练海量数据（各种网页书籍等）去做机器学习，学习文字和文字之间的关联关系（参数！！！），就形成大模型。参数越大，模型越好
- 语言模型只是一个二进制文件，大语言模型文件是那个参数文件，然后一般和一个可以运行启动他的c文件一起，才可以启动模型。当然在现在，一个大模型可能由好几个组成，主要是为了让速度更快损耗更小微调更方便（文件更小的话更方便调整）。不同模型在不同类别的东西上面训练（比如代码电影数学）会不同能力更强，这样结合起来更好。
## 模型推理
- 模型会返回概率最大的预测结果。比如
- ``` 输入我喜欢吃      输出苹果```不返回香蕉是因为训练集中苹果的概率更大一些（储存在大模型里）
## 模型架构
- transformer架构：Google提出，最初是训练出来翻译的，后面发现很适合来做文字预测。底层的大模型都是这个架构。
## 大模型是怎么训练的
1. 文档补充功能：输出一个句子的回复。如我们输入中华人民，他很可能输出共和国。（文档完善器）
2. 今天的大语言模型：可以回答问题，属于生成式模型。从一到二，是除了预训练以后，还加了一些其他的调试。让海量的人来提出问题以及获得答案，所以增加了回答问题的这种能力。比如有一些开源模型已经训练好了，微调：根据自己现有的需要等进行微调。RLHF是把用户得到的反馈来返回给大模型，让大模型下次回答的时候注意。综上，也就是通过微调和强化学习来让他训练的更强。
3. 指令（提示词）工程：把提示词给大模型描述的特别详细，给了很多例子来模拟步骤。和微调有点像，但是是在提示词这个板块。为什么感觉大模型会思考：把每一个词和其位置产生关联
## transformer架构
### 怎么来的
谷歌发的一个论文，叫Attention Is All You Need。起初是为了翻译，但是后来被其他模型所使用。翻译最开始是一个一个词翻译，整体语义不准确。后来通过这个架构关注上下文语境关系，让大语言能够理解前后文翻译的更准确。
！！！！关于transformer基本架构的图和详细讲解-图一
